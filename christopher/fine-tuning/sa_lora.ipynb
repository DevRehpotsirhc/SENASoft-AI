{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "212e5308",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "from transformers import TrainingArguments, Trainer\n",
    "from peft import get_peft_model, LoraConfig, TaskType\n",
    "from datasets import load_dataset\n",
    "import evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "462ad230",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f0234d86af454fbcb36ee48523c8d2ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/39.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\chris\\OneDrive\\Documentos\\SENASoft\\.venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\chris\\.cache\\huggingface\\hub\\models--nlptown--bert-base-multilingual-uncased-sentiment. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd94bfaf76914dc6a1c1c7abcb72214d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/953 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "519f089b22e641199f816d3f1ba51e1f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf6cdea0e5e045478f9516c91d559ab1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28fc71abb3c5409f964ebaf8fc1ec74a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/669M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at nlptown/bert-base-multilingual-uncased-sentiment and are newly initialized because the shapes did not match:\n",
      "- classifier.bias: found shape torch.Size([5]) in the checkpoint and torch.Size([3]) in the model instantiated\n",
      "- classifier.weight: found shape torch.Size([5, 768]) in the checkpoint and torch.Size([3, 768]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "etiquetas = {\n",
    "    0: \"negativo\",\n",
    "    1: \"neutral\",\n",
    "    2: \"positivo\"\n",
    "}\n",
    "\n",
    "modelo_base = \"nlptown/bert-base-multilingual-uncased-sentiment\"\n",
    "tokenizador = AutoTokenizer.from_pretrained(modelo_base)\n",
    "modelo = AutoModelForSequenceClassification.from_pretrained(\n",
    "    modelo_base, \n",
    "    num_labels=3, \n",
    "    ignore_mismatched_sizes=True,\n",
    "    id2label=etiquetas,\n",
    "    label2id={v: k for k, v in etiquetas.items()}\n",
    ")    # Se sitúa en 3 porque el dataset que se cargó usa positivo, negativo y neutal, es decir, 3 etiquetas de clasificación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8ae9af34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 297,219 || all params: 167,655,942 || trainable%: 0.1773\n"
     ]
    }
   ],
   "source": [
    "config_lora = LoraConfig(\n",
    "    task_type = TaskType.SEQ_CLS,   # tipo de tarea en el que se centra el LoRA\n",
    "    r=8,                            # rango de la matriz de entrenamiento (bajo 2 - 8, medio 16 - 32, alto 64 - 128+), a mayor rango mayor consumo\n",
    "    lora_alpha=16,                  # equilibrio entre LoRA y modelo base; a mayor sea el número, LoRA tiene más intervención a partir del dataset sobre el modelo base (se suele usar r*2 o r*4, más del *4 se usa en datasets grandes)\n",
    "    lora_dropout=0.7                # nivel de regulación en LoRA; a menor sea el número, se confía más en el dataset que implementa LoRA (0.0 es confianza plena, > 0.3 es usado en casos de overfitting)\n",
    ")\n",
    "\n",
    "modelo_lora = get_peft_model(modelo, config_lora)\n",
    "modelo_lora.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "768ecb3f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3554586b179344aba407495c2687689c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/857 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\chris\\OneDrive\\Documentos\\SENASoft\\.venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\chris\\.cache\\huggingface\\hub\\datasets--pyupeu--social-media-peruvian-sentiment. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text', 'label', 'label_name'],\n",
      "        num_rows: 9336\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['text', 'label', 'label_name'],\n",
      "        num_rows: 2918\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['text', 'label', 'label_name'],\n",
      "        num_rows: 2335\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "ds = load_dataset(\"pyupeu/social-media-peruvian-sentiment\")     # Nos basamos en comentarios de redes sociales para tomar la forma típica de escribir de un usuario promedio, en este caso de Perú\n",
    "print(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "91d6c43a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizar(batch):\n",
    "    return tokenizador(\n",
    "        batch[\"text\"],          # columna del dataset que contiene el texto\n",
    "        truncation=True,        # si el texto es más largo que el max_lenght, se corta\n",
    "        padding=\"max_length\",   # deja a todos los textos de la misma longitud aunque sean más cortos; se usa para usar tensores de igual tamaño siempre\n",
    "        max_length=128\n",
    "    )\n",
    "\n",
    "ds_tokenizado = ds.map(tokenizar, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1aaa8d3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = TrainingArguments(\n",
    "    output_dir=\"./resultados\",\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=20,\n",
    "    learning_rate=5e-5,\n",
    "    weight_decay=0.01,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"accuracy\",\n",
    "    push_to_hub=False \n",
    ")\n",
    "\n",
    "accuracy_metric = evaluate.load(\"accuracy\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = logits.argmax(axis=-1)\n",
    "    return accuracy_metric.compute(predictions=predictions, references=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cad1b680",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chris\\AppData\\Local\\Temp\\ipykernel_12508\\3271136652.py:1: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='11680' max='11680' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [11680/11680 1:52:30, Epoch 20/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.923900</td>\n",
       "      <td>0.870352</td>\n",
       "      <td>0.604711</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.853300</td>\n",
       "      <td>0.843578</td>\n",
       "      <td>0.627409</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.824200</td>\n",
       "      <td>0.825065</td>\n",
       "      <td>0.641542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.803300</td>\n",
       "      <td>0.806815</td>\n",
       "      <td>0.639400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.785700</td>\n",
       "      <td>0.802350</td>\n",
       "      <td>0.648394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.769500</td>\n",
       "      <td>0.797890</td>\n",
       "      <td>0.656959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.767700</td>\n",
       "      <td>0.784698</td>\n",
       "      <td>0.651392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.752000</td>\n",
       "      <td>0.788920</td>\n",
       "      <td>0.657816</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.742800</td>\n",
       "      <td>0.786437</td>\n",
       "      <td>0.656959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.737600</td>\n",
       "      <td>0.791602</td>\n",
       "      <td>0.657816</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.737200</td>\n",
       "      <td>0.798609</td>\n",
       "      <td>0.648394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.728200</td>\n",
       "      <td>0.797382</td>\n",
       "      <td>0.652248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.727200</td>\n",
       "      <td>0.792685</td>\n",
       "      <td>0.654818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.724900</td>\n",
       "      <td>0.785872</td>\n",
       "      <td>0.656531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.722400</td>\n",
       "      <td>0.785326</td>\n",
       "      <td>0.661670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.714500</td>\n",
       "      <td>0.790850</td>\n",
       "      <td>0.660385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.711800</td>\n",
       "      <td>0.792436</td>\n",
       "      <td>0.656103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.707900</td>\n",
       "      <td>0.789938</td>\n",
       "      <td>0.659957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.706700</td>\n",
       "      <td>0.791962</td>\n",
       "      <td>0.657816</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.710600</td>\n",
       "      <td>0.791305</td>\n",
       "      <td>0.661242</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=11680, training_loss=0.7575733655119595, metrics={'train_runtime': 6752.2725, 'train_samples_per_second': 27.653, 'train_steps_per_second': 1.73, 'total_flos': 1.232475582947328e+16, 'train_loss': 0.7575733655119595, 'epoch': 20.0})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model=modelo_lora,\n",
    "    args=args,\n",
    "    train_dataset=ds_tokenizado[\"train\"],\n",
    "    eval_dataset=ds_tokenizado[\"test\"],\n",
    "    tokenizer=tokenizador, #type:ignore\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
