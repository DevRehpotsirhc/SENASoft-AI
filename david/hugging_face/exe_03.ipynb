{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c0589f4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'route': 'api_help'}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from typing import Any, Dict, List\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model_name = \"katanemo/Arch-Router-1.5B\"\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name, device_map=\"auto\", torch_dtype=\"auto\", trust_remote_code=True\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Please use our provided prompt for best performance\n",
    "TASK_INSTRUCTION = \"\"\"\n",
    "You are a helpful assistant designed to find the best suited route.\n",
    "You are provided with route description within <routes></routes> XML tags:\n",
    "<routes>\n",
    "\n",
    "{routes}\n",
    "\n",
    "</routes>\n",
    "\n",
    "<conversation>\n",
    "\n",
    "{conversation}\n",
    "\n",
    "</conversation>\n",
    "\"\"\"\n",
    "\n",
    "FORMAT_PROMPT = \"\"\"\n",
    "Your task is to decide which route is best suit with user intent on the conversation in <conversation></conversation> XML tags.  Follow the instruction:\n",
    "1. If the latest intent from user is irrelevant or user intent is full filled, response with other route {\"route\": \"other\"}.\n",
    "2. You must analyze the route descriptions and find the best match route for user latest intent. \n",
    "3. You only response the name of the route that best matches the user's request, use the exact name in the <routes></routes>.\n",
    "\n",
    "Based on your analysis, provide your response in the following JSON formats if you decide to match any route:\n",
    "{\"route\": \"route_name\"} \n",
    "\"\"\"\n",
    "\n",
    "# Define route config\n",
    "route_config = [\n",
    "    {\n",
    "        \"name\": \"code_generation\",\n",
    "        \"description\": \"Generating new code snippets, functions, or boilerplate based on user prompts or requirements\",\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"bug_fixing\",\n",
    "        \"description\": \"Identifying and fixing errors or bugs in the provided code across different programming languages\",\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"performance_optimization\",\n",
    "        \"description\": \"Suggesting improvements to make code more efficient, readable, or scalable\",\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"api_help\",\n",
    "        \"description\": \"Assisting with understanding or integrating external APIs and libraries\",\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"programming\",\n",
    "        \"description\": \"Answering general programming questions, theory, or best practices\",\n",
    "    },\n",
    "]\n",
    "\n",
    "# Helper function to create the system prompt for our model\n",
    "def format_prompt(\n",
    "    route_config: List[Dict[str, Any]], conversation: List[Dict[str, Any]]\n",
    "):\n",
    "    return (\n",
    "        TASK_INSTRUCTION.format(\n",
    "            routes=json.dumps(route_config), conversation=json.dumps(conversation)\n",
    "        )\n",
    "        + FORMAT_PROMPT\n",
    "    )\n",
    "\n",
    "# Define conversations\n",
    "\n",
    "conversation = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"what is the Stripe API used for?\",\n",
    "    }\n",
    "]\n",
    "\n",
    "route_prompt = format_prompt(route_config, conversation)\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": route_prompt},\n",
    "]\n",
    "\n",
    "input_ids = tokenizer.apply_chat_template(\n",
    "    messages, add_generation_prompt=True, return_tensors=\"pt\"\n",
    ").to(model.device)\n",
    "\n",
    "# 2. Generate\n",
    "generated_ids = model.generate(\n",
    "    input_ids=input_ids,  # or just positional: model.generate(input_ids, â€¦)\n",
    "    max_new_tokens=32768,\n",
    ")\n",
    "\n",
    "# 3. Strip the prompt from each sequence\n",
    "prompt_lengths = input_ids.shape[1]  # same length for every row here\n",
    "generated_only = [\n",
    "    output_ids[prompt_lengths:]  # slice off the prompt tokens\n",
    "    for output_ids in generated_ids\n",
    "]\n",
    "\n",
    "# 4. Decode if you want text\n",
    "response = tokenizer.batch_decode(generated_only, skip_special_tokens=True)[0]\n",
    "print(response)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
